{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this Notebook with Spark 2 1G kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Users MUST shutdown kernels!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0.cloudera1\n"
     ]
    }
   ],
   "source": [
    "#Ensure we are using the right kernel\n",
    "print (sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import sh\n",
    "import shutil\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File / directory management from Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can operate with files directories using Notebook's connection to Linux and HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 416\r\n",
      "drwxrwxr-x 2 kadochnikov kadochnikov   4096 Nov  4  2016 tom_sawyer_bgs\r\n",
      "-rwxr-xr-x 1 kadochnikov kadochnikov 421884 May  8  2015 tom_sawyer.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l \"/home/kadochnikov/data/gutenberg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘/home/kadochnikov/data/gutenberg/tom_sawyer_wc’: No such file or directory\n",
      "total 416\n",
      "-rwxr-xr-x 1 kadochnikov kadochnikov 421884 May  8  2015 tom_sawyer.txt\n",
      "drwxrwxr-x 2 kadochnikov kadochnikov   4096 Nov  4  2016 tom_sawyer_bgs\n"
     ]
    }
   ],
   "source": [
    "#Using Linux\n",
    "!rm -r \"/home/kadochnikov/data/gutenberg/tom_sawyer_wc\"\n",
    "!ls -lr \"/home/kadochnikov/data/gutenberg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or you can use Python functionality, which allows using variables to encode path names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tom_sawyer.txt', 'tom_sawyer_bgs']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_src = \"/home/kadochnikov/data/gutenberg/\"\n",
    "dir_out = \"/home/kadochnikov/data/gutenberg/tom_sawyer_wc\"\n",
    "\n",
    "os.listdir(dir_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tom_sawyer.txt', 'tom_sawyer_bgs']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Python\n",
    "shutil.rmtree(dir_out, ignore_errors=True)\n",
    "os.listdir(dir_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also choice between shell and Python in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0\n",
      "-rw-r--r--   3 kadochnikov kadochnikov     421884 2018-04-30 14:49 /user/kadochnikov/gutenberg/tom_sawyer.txt\n",
      "drwxr-xr-x   - kadochnikov kadochnikov          0 2018-04-30 14:49 /user/kadochnikov/gutenberg/tom_sawyer_bgs\n",
      "drwxr-xr-x   - kadochnikov kadochnikov          0 2018-04-30 15:09 /user/kadochnikov/gutenberg/tom_sawyer_wc\n",
      "-rw-r--r--   3 kadochnikov kadochnikov          0 2018-04-30 15:09 /user/kadochnikov/gutenberg/tom_sawyer_wc/_SUCCESS\n",
      "-rw-r--r--   3 kadochnikov kadochnikov      73218 2018-04-30 15:09 /user/kadochnikov/gutenberg/tom_sawyer_wc/part-00000\n",
      "-rw-r--r--   3 kadochnikov kadochnikov     143941 2018-04-30 15:09 /user/kadochnikov/gutenberg/tom_sawyer_wc/part-00001\n"
     ]
    }
   ],
   "source": [
    "#Using Linux\n",
    "!hadoop fs -ls -R \"/user/kadochnikov/gutenberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_dir_src = \"hdfs:///user/kadochnikov/gutenberg\"\n",
    "hdfs_dir_out = \"hdfs:///user/kadochnikov/gutenberg/tom_sawyer_wc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 kadochnikov kadochnikov     421884 2018-04-30 14:49 hdfs:///user/kadochnikov/gutenberg/tom_sawyer.txt\n",
      "drwxr-xr-x   - kadochnikov kadochnikov          0 2018-04-30 14:49 hdfs:///user/kadochnikov/gutenberg/tom_sawyer_bgs\n",
      "drwxr-xr-x   - kadochnikov kadochnikov          0 2018-04-30 15:09 hdfs:///user/kadochnikov/gutenberg/tom_sawyer_wc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using Python\n",
    "try:\n",
    "    print(sh.hdfs('dfs','-ls',hdfs_dir_src))\n",
    "except:\n",
    "    print(hdfs_dir_src+' *** Does not exist ***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sh.hdfs('dfs','-rmr',hdfs_dir_out)\n",
    "except:\n",
    "    print(hdfs_dir_out+' *** Does not exist ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Tom Sawyer Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\r",
      "\r\n",
      "The Project Gutenberg EBook of The Adventures of Tom Sawyer, Complete by\r",
      "\r\n",
      "Mark Twain (Samuel Clemens)\r",
      "\r\n",
      "\r",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost\r",
      "\r\n",
      "no restrictions whatsoever. You may copy it, give it away or re-use\r",
      "\r\n",
      "it under the terms of the Project Gutenberg License included with this\r",
      "\r\n",
      "eBook or online at www.gutenberg.net\r",
      "\r\n",
      "\r",
      "\r\n",
      "Title: The Adventures of Tom Sawyer, Complete\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head \"/home/kadochnikov/data/gutenberg/tom_sawyer.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///home/kadochnikov/data/gutenberg/tom_sawyer.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = sc.textFile(\"file:///home/kadochnikov/data/gutenberg/tom_sawyer.txt\")\n",
    "ts.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'The Project Gutenberg EBook of The Adventures of Tom Sawyer, Complete by',\n",
       " 'Mark Twain (Samuel Clemens)',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with almost',\n",
       " 'no restrictions whatsoever. You may copy it, give it away or re-use',\n",
       " 'it under the terms of the Project Gutenberg License included with this',\n",
       " 'eBook or online at www.gutenberg.net',\n",
       " '',\n",
       " 'Title: The Adventures of Tom Sawyer, Complete']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 12, 4, 0, 15, 12, 12, 5, 0, 7]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.map(lambda line: len(line.split())).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 12, 4, 1, 15, 12, 12, 5, 1, 7]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split with space treats empty lines as one-word lines.  Go figure...\n",
    "ts.map(lambda line: len(line.split(\" \"))).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.map(lambda line: len(line.split())).\\\n",
    "reduce(lambda a, b: a if (a > b) else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts = ts.flatMap(lambda line: line.split()).\\\n",
    "map(lambda word: (word, 1)).\\\n",
    "reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wordCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the flatMap, map, and the reduceByKey functions to do a word count of each word in the Tom Sawyer book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note the familiar \"parts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 414), ('Project', 78), ('EBook', 1), ('of', 1554), ('Tom', 453)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts = ts.flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a+b)\n",
    "wordCounts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"\\'Bout', 1),\n",
       " ('\"\\'Deed', 1),\n",
       " ('\"\\'My', 1),\n",
       " ('\"\\'Nuff!\"', 1),\n",
       " ('\"\\'Tain\\'t', 3)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sort in Ascending Order\n",
    "wordCounts.sortByKey(ascending=True).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zephyr', 1),\n",
       " ('zenith', 1),\n",
       " ('zebras--all', 1),\n",
       " ('zeal', 1),\n",
       " ('youthful', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sort in Descending Order\n",
    "wordCounts.sortByKey(ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Happy with sorted results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3481, 'the'),\n",
       " (2921, 'and'),\n",
       " (1768, 'a'),\n",
       " (1750, 'to'),\n",
       " (1554, 'of'),\n",
       " (1123, 'was'),\n",
       " (935, 'in'),\n",
       " (849, 'he'),\n",
       " (787, 'that'),\n",
       " (774, 'his')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountsSorted = wordCounts.map(lambda x:(x[1],x[0])).sortByKey(ascending=False)\n",
    "wordCountsSorted.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving results into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountsSorted.saveAsTextFile(hdfs_dir_out) \n",
    "#Won't work due to a \"feature\" in our Spark configuration\n",
    "#wordCounts.saveAsTextFile(\"/home/kadochnikov/data/gutenberg/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 kadochnikov kadochnikov          0 2018-04-30 15:44 hdfs:///user/kadochnikov/gutenberg/tom_sawyer_wc/_SUCCESS\n",
      "-rw-r--r--   3 kadochnikov kadochnikov      73218 2018-04-30 15:44 hdfs:///user/kadochnikov/gutenberg/tom_sawyer_wc/part-00000\n",
      "-rw-r--r--   3 kadochnikov kadochnikov     143941 2018-04-30 15:44 hdfs:///user/kadochnikov/gutenberg/tom_sawyer_wc/part-00001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(sh.hdfs('dfs','-ls',hdfs_dir_out))\n",
    "except:\n",
    "    print(hdfs_dir_out+' *** Does not exist ***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"(3481, 'the')\",\n",
       " \"(2921, 'and')\",\n",
       " \"(1768, 'a')\",\n",
       " \"(1750, 'to')\",\n",
       " \"(1554, 'of')\",\n",
       " \"(1123, 'was')\",\n",
       " \"(935, 'in')\",\n",
       " \"(849, 'he')\",\n",
       " \"(787, 'that')\",\n",
       " \"(774, 'his')\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_saved = sc.textFile(hdfs_dir_out)\n",
    "word_counts_saved.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some NLP Operations.  Examples of Word and N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first problem is that values in each partition of our initial RDD describe lines from the file rather than sentences. Sentences may be split over multiple lines. The glom() RDD method is used to create a single entry for each document containing the list of all lines, we can then join the lines up, then resplit them into sentences using \".\" as the separator, using flatMap so that every object in our RDD is now a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sc.textFile(\"file:///home/kadochnikov/data/gutenberg/tom_sawyer.txt\") \\\n",
    "    .glom() \\\n",
    "    .map(lambda x: \" \".join(x)) \\\n",
    "    .flatMap(lambda x: x.split(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have isolated each sentence we can split it into a list of words and extract the word bigrams from it. Our new RDD contains tuples containing the word bigram (itself a tuple containing the first and second word) as the first value and the number 1 as the second value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = sentences.map(lambda x:x.split()) \\\n",
    "    .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally we can apply the same reduceByKey and sort steps that we used in the wordcount example, to count up the bigrams and sort them in order of descending frequency. In reduceByKey the key is not an individual word but a bigram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(382, ('of', 'the')),\n",
       " (309, ('in', 'the')),\n",
       " (178, ('to', 'the')),\n",
       " (176, ('and', 'the')),\n",
       " (112, ('with', 'a')),\n",
       " (112, ('was', 'a')),\n",
       " (110, ('he', 'was')),\n",
       " (110, ('it', 'was')),\n",
       " (101, ('and', 'then')),\n",
       " (101, ('in', 'a'))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bigrams = bigrams.reduceByKey(lambda x,y:x+y) \\\n",
    "    .map(lambda x:(x[1],x[0])) \\\n",
    "    .sortByKey(ascending=False)\n",
    "freq_bigrams.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating bigrams, trigrams and fourgrams in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sc.textFile(\"file:///home/kadochnikov/data/gutenberg/tom_sawyer.txt\") \\\n",
    "    .glom() \\\n",
    "    .map(lambda x: \" \".join(x)) \\\n",
    "    .map(lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower()) \\\n",
    "    .map(lambda x: x.replace('gutenberg','').replace('project','').replace('literary','').replace('archive','').replace('foundation','')) \\\n",
    "    .flatMap(lambda x: x.split(\".\"))\n",
    "    \n",
    "sentences.cache()\n",
    "\n",
    "\n",
    "bigrams = sentences.map(lambda x:x.split()).\\\n",
    "    flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)]).\\\n",
    "    reduceByKey(lambda x,y:x+y).\\\n",
    "    map(lambda x:(x[1],x[0])).sortByKey(ascending=False)\n",
    "    \n",
    "trigrams = sentences.map(lambda x:x.split()).\\\n",
    "    flatMap(lambda x: [((x[i],x[i+1],x[i+2]),1) for i in range(0,len(x)-2)]).\\\n",
    "    reduceByKey(lambda x,y:x+y).\\\n",
    "    map(lambda x:(x[1],x[0])).sortByKey(ascending=False)\n",
    "    \n",
    "fourgrams = sentences.map(lambda x:x.split()).\\\n",
    "    flatMap(lambda x: [((x[i],x[i+1],x[i+2],x[i+3]),1) for i in range(0,len(x)-3)]).\\\n",
    "    reduceByKey(lambda x,y:x+y).\\\n",
    "    map(lambda x:(x[1],x[0])).sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(385, ('of', 'the')),\n",
       " (319, ('in', 'the')),\n",
       " (187, ('and', 'the')),\n",
       " (181, ('to', 'the')),\n",
       " (173, ('it', 'was')),\n",
       " (148, ('he', 'was')),\n",
       " (125, ('and', 'then')),\n",
       " (117, ('was', 'a')),\n",
       " (117, ('he', 'had')),\n",
       " (116, ('there', 'was'))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, ('there', 'was', 'a')),\n",
       " (31, ('by', 'and', 'by')),\n",
       " (25, ('there', 'was', 'no')),\n",
       " (22, ('out', 'of', 'the')),\n",
       " (18, ('it', 'was', 'a')),\n",
       " (17, ('he', 'did', 'not')),\n",
       " (16, ('he', 'had', 'been')),\n",
       " (15, ('to', 'be', 'a')),\n",
       " (15, ('out', 'of', 'his')),\n",
       " (14, ('and', 'began', 'to'))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, ('the', 'rest', 'of', 'the')),\n",
       " (9, ('terms', 'of', 'this', 'agreement')),\n",
       " (8, ('the', 'terms', 'of', 'this')),\n",
       " (7, ('the', 'middle', 'of', 'the')),\n",
       " (7, ('from', 'time', 'to', 'time')),\n",
       " (6, ('a', 'moment', 'and', 'then')),\n",
       " (6, ('but', 'there', 'was', 'no')),\n",
       " (6, ('then', 'there', 'was', 'a')),\n",
       " (6, ('at', 'the', 'end', 'of')),\n",
       " (6, ('i', \"don't\", 'want', 'to'))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourgrams.take(10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pySpark 2.2.0 1G",
   "language": "python",
   "name": "pyspark2_1g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
