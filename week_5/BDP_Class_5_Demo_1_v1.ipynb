{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this Notebook with Spark 2 1G kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Users MUST shutdown kernels!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0.cloudera1\n"
     ]
    }
   ],
   "source": [
    "#Ensure we are using the right kernel\n",
    "print (sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Airlines Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read two CSV files from Linux space and count lines.  Note execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14462945\n",
      "--- 21.53054118156433 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "readme_linux = sc.textFile(\"file:///home/kadochnikov/data/200*.csv\")\n",
    "print(readme_linux.count())\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can execute your Linux scripts directly from the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x 1 kadochnikov kadochnikov 702878193 Apr  9  2015 /home/kadochnikov/data/2007.csv\r\n",
      "-rwxr-xr-x 1 kadochnikov kadochnikov 689413344 Apr 10  2015 /home/kadochnikov/data/2008.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /home/kadochnikov/data/200*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read two CSV files from HDFS and count lines.  Note execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14462945\n",
      "--- 5.052244663238525 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "readme_hdfs = sc.textFile(\"hdfs:///user/kadochnikov/Airlines/200*.csv\")\n",
    "print(readme_hdfs.count())\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can execute your Linux > HDFS scripts directly from the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0\n",
      "-rw-r--r--   3 kadochnikov kadochnikov  702878193 2017-03-13 17:15 /user/kadochnikov/Airlines/2007.csv\n",
      "-rw-r--r--   3 kadochnikov kadochnikov  689413344 2017-03-13 17:16 /user/kadochnikov/Airlines/2008.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/kadochnikov/Airlines/200*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read single CSV file from HDFS space and count lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7453216"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_raw = sc.textFile(\"hdfs:///user/kadochnikov/Airlines/2007.csv\")\n",
    "readme_raw.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display first line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay',\n",
       " '2007,1,1,1,1232,1225,1341,1340,WN,2891,N351,69,75,54,1,7,SMF,ONT,389,4,11,0,,0,0,0,0,0,0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_raw.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the filter transformation to return a new RDD without header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7453215"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme_raw = sc.textFile(\"hdfs:///user/kadochnikov/Airlines/2007.csv\")\n",
    "readme = readme_raw.mapPartitionsWithIndex(lambda i, iter: islice(iter, 1, None) if i == 0 else iter) #filter out header\n",
    "readme.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2007,1,1,1,1232,1225,1341,1340,WN,2891,N351,69,75,54,1,7,SMF,ONT,389,4,11,0,,0,0,0,0,0,0',\n",
       " '2007,1,1,1,1918,1905,2043,2035,WN,462,N370,85,90,74,8,13,SMF,PDX,479,5,6,0,,0,0,0,0,0,0',\n",
       " '2007,1,1,1,2206,2130,2334,2300,WN,1229,N685,88,90,73,34,36,SMF,PDX,479,6,9,0,,0,3,0,0,0,31',\n",
       " '2007,1,1,1,1230,1200,1356,1330,WN,1355,N364,86,90,75,26,30,SMF,PDX,479,3,8,0,,0,23,0,0,0,3',\n",
       " '2007,1,1,1,831,830,957,1000,WN,2278,N480,86,90,74,-3,1,SMF,PDX,479,3,9,0,,0,0,0,0,0,0',\n",
       " '2007,1,1,1,1430,1420,1553,1550,WN,2386,N611SW,83,90,74,3,10,SMF,PDX,479,2,7,0,,0,0,0,0,0,0',\n",
       " '2007,1,1,1,1936,1840,2217,2130,WN,409,N482,101,110,89,47,56,SMF,PHX,647,5,7,0,,0,46,0,0,0,1',\n",
       " '2007,1,1,1,944,935,1223,1225,WN,1131,N749SW,99,110,86,-2,9,SMF,PHX,647,4,9,0,,0,0,0,0,0,0',\n",
       " '2007,1,1,1,1537,1450,1819,1735,WN,1212,N451,102,105,90,44,47,SMF,PHX,647,5,7,0,,0,20,0,0,0,24',\n",
       " '2007,1,1,1,1318,1315,1603,1610,WN,2456,N630WN,105,115,92,-7,3,SMF,PHX,647,5,8,0,,0,0,0,0,0,0']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the filter transformation to return a new RDD with a subset of the items in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_with_ord = readme.filter(lambda line: \"ORD\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2007,1,25,4,1052,1100,1359,1414,XE,1202,N12167,127,134,105,-15,-8,ORD,EWR,719,5,17,0,,0,0,0,0,0,0',\n",
       " '2007,1,20,6,845,845,1015,1020,XE,1201,N14153,150,155,123,-5,0,EWR,ORD,719,7,20,0,,0,0,0,0,0,0',\n",
       " '2007,1,28,7,1541,1500,1811,1750,XE,2836,N12163,150,170,128,21,41,ORD,IAH,925,7,15,0,,0,0,0,5,0,16',\n",
       " '2007,1,27,6,837,845,1013,1020,XE,1201,N12157,156,155,122,-7,-8,EWR,ORD,719,11,23,0,,0,0,0,0,0,0',\n",
       " '2007,1,29,1,2045,2000,2305,2211,XE,2701,N14508,80,71,52,54,45,ORD,CLE,316,9,19,0,,0,0,0,9,0,45',\n",
       " '2007,1,16,2,1044,1045,1229,1220,XE,1203,N14993,165,155,127,9,-1,EWR,ORD,719,8,30,0,,0,0,0,0,0,0',\n",
       " '2007,1,17,3,1851,1900,2204,2220,XE,1206,N19554,133,140,109,-16,-9,ORD,EWR,719,11,13,0,,0,0,0,0,0,0',\n",
       " '2007,1,12,5,2045,1745,2256,2003,XE,2023,N15973,71,78,45,173,180,ORD,CLE,316,6,20,0,,0,0,0,0,0,173',\n",
       " '2007,1,12,5,959,930,1247,1212,XE,2324,N11109,168,162,141,35,29,ORD,IAH,925,8,19,0,,0,0,0,6,0,29',\n",
       " '2007,1,26,5,2035,2000,2242,2211,XE,2701,N16510,67,71,46,31,35,ORD,CLE,316,6,15,0,,0,0,0,0,0,31']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_ord.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain together transformations and actions. To find out how many lines contains \"ORD\", type in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "751500"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.filter(lambda line: \"ORD\" in line).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caching Airlines file to perform multiple filter operations on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[21] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_raw = sc.textFile(\"hdfs:///user/kadochnikov/Airlines/2007.csv\")\n",
    "airlines = airlines_raw.mapPartitionsWithIndex(lambda i, iter: islice(iter, 1, None) if i == 0 else iter)\n",
    "airlines_orig_dest = airlines.map(lambda s: s.split(\",\")[16:18]) \n",
    "airlines_orig_dest.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SMF', 'ONT'], ['SMF', 'PDX'], ['SMF', 'PDX'], ['SMF', 'PDX'], ['SMF', 'PDX'], ['SMF', 'PDX'], ['SMF', 'PHX'], ['SMF', 'PHX'], ['SMF', 'PHX'], ['SMF', 'PHX']]\n",
      "--- 10.786569833755493 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#First execution builds cache\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print (airlines_orig_dest.take(10))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SMF', 'ONT'], ['SMF', 'PDX'], ['SMF', 'PDX'], ['SMF', 'PDX'], ['SMF', 'PDX'], ['SMF', 'PDX'], ['SMF', 'PHX'], ['SMF', 'PHX'], ['SMF', 'PHX'], ['SMF', 'PHX']]\n",
      "--- 0.1302471160888672 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Second execution uses cached data\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print (airlines_orig_dest.take(10))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "751500"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_orig_dest.filter(lambda line: \"ORD\" in line).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252704"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_orig_dest.filter(lambda line: \"JFK\" in line).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ORD', 'EWR'],\n",
       " ['EWR', 'ORD'],\n",
       " ['ORD', 'IAH'],\n",
       " ['EWR', 'ORD'],\n",
       " ['ORD', 'CLE'],\n",
       " ['EWR', 'ORD'],\n",
       " ['ORD', 'EWR'],\n",
       " ['ORD', 'CLE'],\n",
       " ['ORD', 'IAH'],\n",
       " ['ORD', 'CLE']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_orig_dest.filter(lambda line: \"ORD\" in line).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['JFK', 'CLE'],\n",
       " ['CLE', 'JFK'],\n",
       " ['JFK', 'CLE'],\n",
       " ['JFK', 'CLE'],\n",
       " ['CLE', 'JFK'],\n",
       " ['JFK', 'CLE'],\n",
       " ['JFK', 'CLE'],\n",
       " ['JFK', 'CLE'],\n",
       " ['CLE', 'JFK'],\n",
       " ['CLE', 'JFK']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_orig_dest.filter(lambda line: \"JFK\" in line).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Release the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[21] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_orig_dest.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create two RDDs: one for origination and second for destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_raw = sc.textFile(\"hdfs:///user/kadochnikov/Airlines/2007.csv\")\n",
    "#airlines = airlines_raw.mapPartitionsWithIndex(lambda i, iter: islice(iter, 1, None) if i == 0 else iter)\n",
    "airlines_orig = airlines_raw.map(lambda s: s.split(\",\")[16]) \n",
    "airlines_dest = airlines_raw.map(lambda s: s.split(\",\")[17]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Origin'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_orig.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dest'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_dest.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this map invocation, we use a function which replaces each original value in the input RDD with a 2-tuple containing the value (origin) in the first position and the integer value 1 in the second position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = airlines_orig.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Origin', 1),\n",
       " ('SMF', 1),\n",
       " ('SMF', 1),\n",
       " ('SMF', 1),\n",
       " ('SMF', 1),\n",
       " ('SMF', 1),\n",
       " ('SMF', 1),\n",
       " ('SMF', 1),\n",
       " ('SMF', 1),\n",
       " ('SMF', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here the key will be the word and lambda function will sum up the word counts for each word. The output RDD will consist of a single tuple for each unique word in the data, where the word is stored at the first position in the tuple and the word count is stored at the second position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ONT', 41643),\n",
       " ('PDX', 58603),\n",
       " ('SWF', 5592),\n",
       " ('AUS', 51560),\n",
       " ('BQN', 1370),\n",
       " ('FLL', 69414),\n",
       " ('LGB', 14210),\n",
       " ('DCA', 89666),\n",
       " ('IAH', 200420),\n",
       " ('DTW', 177478)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 = a1.reduceByKey(lambda x,y:x+y)\n",
    "a2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map a lambda function to the data which will swap over the first and second values in each tuple, now the word count appears in the first position and the word in the second position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(177478, 'DTW'),\n",
       " (58603, 'PDX'),\n",
       " (200420, 'IAH'),\n",
       " (51560, 'AUS'),\n",
       " (89666, 'DCA'),\n",
       " (69414, 'FLL'),\n",
       " (41643, 'ONT'),\n",
       " (5701, 'ABE'),\n",
       " (23535, 'BHM'),\n",
       " (6171, 'MOB')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3 = a2.map(lambda x:(x[1],x[0]))\n",
    "a3.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort the input RDD by the key value (the value at the first position in each tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(413851, 'ATL'),\n",
       " (375784, 'ORD'),\n",
       " (297345, 'DFW'),\n",
       " (240928, 'DEN'),\n",
       " (237597, 'LAX'),\n",
       " (211072, 'PHX'),\n",
       " (200420, 'IAH'),\n",
       " (183668, 'LAS'),\n",
       " (177478, 'DTW'),\n",
       " (155846, 'MSP')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a4 = a3.sortByKey(ascending=False)\n",
    "a4.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate frequencies by airport for origination and destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_orig = airlines_orig.map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y)\n",
    "a_dest = airlines_dest.map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The join function combines the two datasets (K,V) and (K,W) together and get (K, (V,W))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[62] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_orig_dest = a_orig.join(a_dest)\n",
    "a_orig_dest.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AUS', (51560, 51579)),\n",
       " ('BHM', (23535, 23540)),\n",
       " ('DTW', (177478, 177471)),\n",
       " ('FLL', (69414, 69444)),\n",
       " ('ONT', (41643, 41644)),\n",
       " ('PDX', (58603, 58634)),\n",
       " ('AEX', (2996, 2988)),\n",
       " ('IAH', (200420, 200428)),\n",
       " ('BFL', (5195, 5201)),\n",
       " ('LRD', (2543, 2542))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_orig_dest.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the values together to get the total count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_orig_dest_sum = a_orig_dest.map(lambda k: (k[0], (k[1][0]+k[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AUS', 103139),\n",
       " ('BHM', 47075),\n",
       " ('DTW', 354949),\n",
       " ('FLL', 138858),\n",
       " ('ONT', 83287),\n",
       " ('PDX', 117237),\n",
       " ('AEX', 5984),\n",
       " ('IAH', 400848),\n",
       " ('BFL', 10396),\n",
       " ('LRD', 5085)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_orig_dest_sum.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[112] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Free-up memory\n",
    "a_orig_dest.unpersist()\n",
    "airlines_orig.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduceByKey in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the results of map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Origin', 1), ('SMF', 1), ('SMF', 1), ('SMF', 1), ('SMF', 1), ('SMF', 1), ('SMF', 1), ('SMF', 1), ('SMF', 1), ('SMF', 1)]\n"
     ]
    }
   ],
   "source": [
    "a1 = airlines_orig.map(lambda x: (x, 1)).cache()\n",
    "print (a1.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying a \"textbook\" variant of reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DTW', 177478), ('PDX', 58603), ('IAH', 200420), ('AUS', 51560), ('DCA', 89666), ('FLL', 69414), ('ONT', 41643), ('ABE', 5701), ('BHM', 23535), ('MOB', 6171)]\n"
     ]
    }
   ],
   "source": [
    "b1 = a1.reduceByKey(lambda x,y:x+y)\n",
    "print (b1.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing (x+y) with a traditional (x+1) counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AUS', 9028), ('BHM', 4545), ('DTW', 31171), ('FLL', 13269), ('ONT', 7822), ('PDX', 11147), ('AEX', 841), ('IAH', 44626), ('BFL', 1377), ('LRD', 573)]\n"
     ]
    }
   ],
   "source": [
    "b2 = a1.reduceByKey(lambda x,y:x+1)\n",
    "print (b2.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we use (y+1) as counter instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('IAH', 3), ('LRD', 3), ('LCH', 3), ('DCA', 3), ('AUS', 3), ('MOB', 3), ('DTW', 3), ('BHM', 3), ('SHV', 3), ('ONT', 3)]\n"
     ]
    }
   ],
   "source": [
    "b3 = a1.reduceByKey(lambda x,y:y+1)\n",
    "print (b3.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now back to the textbook example of (x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DCA', 89666), ('DTW', 177478), ('FLL', 69414), ('IAH', 200420), ('ONT', 41643), ('PDX', 58603), ('ABE', 5701), ('AUS', 51560), ('BHM', 23535), ('MOB', 6171)]\n"
     ]
    }
   ],
   "source": [
    "b4 = a1.reduceByKey(lambda x,y:x+y)\n",
    "print (b4.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare results for one airport (Gerald R. Ford International Airport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('GRR', 16412)]\n",
      "[('GRR', 697)]\n",
      "[('GRR', 3)]\n"
     ]
    }
   ],
   "source": [
    "#Reduced by x+y\n",
    "c1 = b1.filter(lambda line: \"GRR\" in line)\n",
    "print (c1.take(10))\n",
    "\n",
    "#Reduced by x+1\n",
    "c2 = b2.filter(lambda line: \"GRR\" in line)\n",
    "print (c2.take(10))\n",
    "\n",
    "#Reduced by y+1\n",
    "c3 = b3.filter(lambda line: \"GRR\" in line)\n",
    "print (c3.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the highest possible number of flights for any airport using (y+1) counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 'XNA'), (3, 'MKE'), (3, 'GSP'), (3, 'RIC'), (3, 'PVD'), (3, 'IAD'), (3, 'LIT'), (3, 'SAV'), (3, 'CMH'), (3, 'PIT')]\n"
     ]
    }
   ],
   "source": [
    "#Reduced by y+1\n",
    "d1 = b3.map(lambda x:(x[1],x[0]))\n",
    "d2 = d1.sortByKey(ascending=False)\n",
    "\n",
    "print (d2.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the highest results for proper (x+y) counter.  Do they look close to the US busiest airports?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(413851, 'ATL'), (375784, 'ORD'), (297345, 'DFW'), (240928, 'DEN'), (237597, 'LAX'), (211072, 'PHX'), (200420, 'IAH'), (183668, 'LAS'), (177478, 'DTW'), (155846, 'MSP')]\n"
     ]
    }
   ],
   "source": [
    "#Reduced by x+1\n",
    "d3 = b1.map(lambda x:(x[1],x[0]))\n",
    "d4 = d3.sortByKey(ascending=False)\n",
    "\n",
    "print (d4.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  reduceByKey explanation\n",
    "\n",
    "reduceByKey is an associative and commutative reduce function\n",
    "reduceByKey operates by merging the values for each key using an associative and commutative reduce function.  \n",
    "It applies the function first within each partition on a per-key basis and then across the partitions, allowing it to scale efficiently to large datasets\n",
    "(x+1) only provides us with results within one of the partitions\n",
    "(y+1) only provides us with results across partitions (max value = # of partitions)\n",
    "(x+y) combines results within each partition with results across all partitions, by computing local sums for each key in each partition and combining those local sums into larger sums after shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results\n",
    "Spark (just like Pig) expects empty directory and will err-out if the directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0\n",
      "Found 3 items\n",
      "-rw-r--r--   3 kadochnikov kadochnikov          0 2018-04-29 17:50 /user/kadochnikov/temp/_SUCCESS\n",
      "-rw-r--r--   3 kadochnikov kadochnikov      19475 2018-04-29 17:50 /user/kadochnikov/temp/part-00000\n",
      "-rw-r--r--   3 kadochnikov kadochnikov      19394 2018-04-29 17:50 /user/kadochnikov/temp/part-00001\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls \"/user/kadochnikov/temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0\n",
      "2018-04-29 20:18:03,256 INFO  [main] fs.TrashPolicyDefault (TrashPolicyDefault.java:moveToTrash(163)) - Moved: 'hdfs://nameservice1/user/kadochnikov/temp' to trash at: hdfs://nameservice1/user/kadochnikov/.Trash/Current/user/kadochnikov/temp\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r \"/user/kadochnikov/temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4.saveAsTextFile(\"/user/kadochnikov/temp/\")\n",
    "#Won't work due to bug (or feature) in our Spark configuration\n",
    "#wordCounts.saveAsTextFile(\"/home/kadochnikov/temp/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir '/home/kadochnikov/temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0\r\n"
     ]
    }
   ],
   "source": [
    "#Getmerge will overwrite the results\n",
    "!hadoop fs -getmerge '/user/kadochnikov/temp' '/home/kadochnikov/temp/air_frequency.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 32\r\n",
      "-rw-r--r-- 1 kadochnikov kadochnikov 4332 Apr 29 20:21 air_frequency.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l '/home/kadochnikov/temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0\n",
      "rm: `/user/kadochnikov/test': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#Cleaning-up to eliminate wasted disk space\n",
    "!hadoop fs -rm -r \"/user/kadochnikov/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[112] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Releasing memory\n",
    "airlines_orig.unpersist()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
