{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "from IPython.core.display import display, HTML\n",
    "import pandas as pd\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('max_colwidth',100)    \n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8 |Anaconda custom (64-bit)| (default, Dec 30 2018, 01:22:34) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import json\n",
    "from itertools import combinations, takewhile\n",
    "import collections\n",
    "\n",
    "from simhash import Simhash, SimhashIndex\n",
    "\n",
    "sns.set()\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0-cdh6.1.0\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "# from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "# OneHotEncoderEstimator is available starting from Spark 2.3\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -h '/user/ivy2/Tweets/' > '/home/sriharis/git_projects/BigDataEngg/final_project/file_list.txt'\n",
    "tweets_path = '/user/ivy2/Tweets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_item_to_list(arr, val, unique=False):\n",
    "    if val in arr:\n",
    "        if not unique:\n",
    "            return arr.extend([val])\n",
    "    if val not in arr:\n",
    "        return arr.extend([val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "\n",
    "def read_all_lines(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            start_loc = line.find('/user/ivy2/Tweets/')\n",
    "            if start_loc < 0:\n",
    "                continue\n",
    "            all_files.append(line[start_loc:].strip())\n",
    "    \n",
    "read_all_lines('./file_list.txt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/user/ivy2/Tweets/tweets201706221015.json'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'201706221015'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = all_files[0]\n",
    "s = '/tweets'\n",
    "l = a.find(s)\n",
    "a[l + len(s):-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placeholder_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashtags = {\n",
    "    \"uchicago\": [],\n",
    "    \"upenn\": []\n",
    "}\n",
    "\n",
    "uc_favored_tags = [\"uchicago\", \"uchearing\", \"uchicagostudents\", \"uchicagomedicine\", \"universityofchicago\",\n",
    "                   \"uchicagonsi\", \"uchicagotoday\", \"pritzkerschoolofmedicine\", \"uchicagoarts\", \n",
    "                   \"uofc\", \"uchicagoalumni\",\"uchicagograham\", \"maroonmade\", \"uchicagompcs\", \"chicagobooth\"]\n",
    "\n",
    "upenn_favored_tags = [\"upenn\", \"penn\", \"uofpenn\", \"universityofpennsylvania\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_to_keep = [\"id_str\", \n",
    "                  \"text\",\n",
    "                  \"in_reply_to_status_id_str\",\n",
    "                  \"in_reply_to_user_id_str\", \n",
    "                  \"created_at\",\n",
    "                  # User columns\n",
    "                  \"user.id_str\",\n",
    "                  \"user.name\",\n",
    "                  \"user.followers_count\",\n",
    "                  \"user.favourites_count\",\n",
    "                  \"user.statuses_count\",\n",
    "                  \"user.friends_count\",\n",
    "                  # Other attributes\n",
    "                  \"coordinates\",\n",
    "                  \"favorite_count\",\n",
    "                  \"entities.hashtags\",\n",
    "                  \"favorited\", \n",
    "                  # Retweet columns\n",
    "                  \"retweet_count\", \n",
    "                  \"retweeted\",\n",
    "                  \"retweeted_status.user.id_str\",\n",
    "                  \"retweeted_status.user.name\"\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/user/ivy2/Tweets/tweets201706221015.json'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the relevant hashtags and populate a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets_df = None\n",
    "\n",
    "counter = 10\n",
    "\n",
    "fixed_col_names = [\n",
    "    \"id_str\", \n",
    "    \"text\",\n",
    "    \"in_reply_to_status_id_str\",\n",
    "    \"in_reply_to_user_id_str\", \n",
    "    \"created_at\",\n",
    "    # User columns\n",
    "    \"user_id_str\",\n",
    "    \"user_name\",\n",
    "    \"user_followers_count\",\n",
    "    \"user_favourites_count\",\n",
    "    \"user_statuses_count\",\n",
    "    \"user_friends_count\",\n",
    "    # Other attributes\n",
    "    \"coordinates\",\n",
    "    \"favorite_count\",\n",
    "    \"entities_hashtags\",\n",
    "    \"favorited\", \n",
    "    # Retweet columns\n",
    "    \"retweet_count\", \n",
    "    \"retweeted\",\n",
    "    \"retweeted_status_user_id_str\",\n",
    "    \"retweeted_status_user_name\"\n",
    "]\n",
    "\n",
    "\n",
    "for file in all_files:\n",
    "    df = spark.read.json('hdfs://'+file)\n",
    "    tmp_df = df.select(fields_to_keep).toPandas()\n",
    "    tmp_df.columns = fixed_col_names\n",
    "    \n",
    "    def clean_hashtags(row):\n",
    "        if row is None:\n",
    "            return []\n",
    "        if row is np.NaN:\n",
    "            return []\n",
    "        if (len(row) == 0):\n",
    "            return []\n",
    "        tags = []\n",
    "        for item in row:\n",
    "            tags.append(item.text)\n",
    "        return tags\n",
    "    \n",
    "    tmp_df[\"hashtags_cleaned\"] = tmp_df[\"entities_hashtags\"].apply(clean_hashtags)\n",
    "    \n",
    "    # Add a date column by parsing the file name\n",
    "    s = '/tweets'\n",
    "    l = file.find(s)\n",
    "    timestamp = file[l + len(s):-5]\n",
    "    tmp_df[\"scraped_timestamp\"] = timestamp\n",
    "    \n",
    "    # ----------- U Chicago\n",
    "    \n",
    "    def is_uc_tweet(row):\n",
    "        for ftag in uc_favored_tags:\n",
    "            for tag in row:\n",
    "                if ftag.lower() in tag.lower():\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    tmp_df[\"uc_tweet\"] = tmp_df[\"hashtags_cleaned\"].apply(is_uc_tweet)\n",
    "    \n",
    "    # ----------- U Penn\n",
    "    \n",
    "    if tweets_df is None:\n",
    "        tweets_df = pd.DataFrame(columns=tmp_df.columns)\n",
    "        \n",
    "    tweets_df = tweets_df.append(tmp_df[tmp_df[\"uc_tweet\"] == True], ignore_index=True)\n",
    "    \n",
    "    counter -= 1\n",
    "    if counter <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    tweets_df.shape,\n",
    "    tweets_df.head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.iloc[0,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('hdfs:///user/ivy2/Tweets/tweets201706221015.json')\n",
    "df.cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df.select(fields_to_keep).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean up hashtag column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtags(row):\n",
    "    if row is None:\n",
    "        return []\n",
    "    if row is np.NaN:\n",
    "        return []\n",
    "    if (len(row) == 0):\n",
    "        return []\n",
    "    tags = []\n",
    "    for item in row:\n",
    "        tags.append(item.text)\n",
    "    return tags\n",
    "tmp_df[\"hashtags_cleaned\"] = tmp_df[\"hashtags\"].apply(clean_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if our favored hashtags exist in these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_uc_tweet(row):\n",
    "    for ftag in uc_favored_tags:\n",
    "        for tag in row:\n",
    "            if ftag in tag:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "tmp_df[\"uc_tweet\"] = tmp_df[\"hashtags_cleaned\"].apply(is_uc_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_upenn_tweet(row):\n",
    "    for ftag in upenn_favored_tags:\n",
    "        for tag in row:\n",
    "            if ftag in tag:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "tmp_df[\"upenn_tweet\"] = tmp_df[\"hashtags_cleaned\"].apply(is_upenn_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df[tmp_df[\"uc_tweet\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df[tmp_df[\"upenn_tweet\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for hashlist in tmp_df[\"hashtags_cleaned\"]:\n",
    "#     for tag in hashlist:\n",
    "#         add_item_to_list(all_hashtags[\"uchicago\"], tag, unique=True)\n",
    "# all_hashtags[\"uchicago\"].sort()\n",
    "# print(len(all_hashtags[\"uchicago\"]))\n",
    "\n",
    "# uc_tags = []\n",
    "# for ftag in uc_favored_tags:\n",
    "#     for tag in all_hashtags:\n",
    "#         if ftag in tag:\n",
    "#             uc_tags.append(tag) \n",
    "\n",
    "# uc_tags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark 2G 4e",
   "language": "python",
   "name": "pyspark2_2g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
